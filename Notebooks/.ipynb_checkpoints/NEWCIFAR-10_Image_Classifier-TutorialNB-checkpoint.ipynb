{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this project, you will build a neural network of your own design to evaluate the CIFAR-10 dataset.\n",
    "Our target accuracy is 70%, but any accuracy over 50% is a great start.\n",
    "Some of the benchmark results on CIFAR-10 include:\n",
    "\n",
    "78.9% Accuracy | [Deep Belief Networks; Krizhevsky, 2010](https://www.cs.toronto.edu/~kriz/conv-cifar10-aug2010.pdf)\n",
    "\n",
    "90.6% Accuracy | [Maxout Networks; Goodfellow et al., 2013](https://arxiv.org/pdf/1302.4389.pdf)\n",
    "\n",
    "96.0% Accuracy | [Wide Residual Networks; Zagoruyko et al., 2016](https://arxiv.org/pdf/1605.07146.pdf)\n",
    "\n",
    "99.0% Accuracy | [GPipe; Huang et al., 2018](https://arxiv.org/pdf/1811.06965.pdf)\n",
    "\n",
    "98.5% Accuracy | [Rethinking Recurrent Neural Networks and other Improvements for ImageClassification; Nguyen et al., 2020](https://arxiv.org/pdf/2007.15161.pdf)\n",
    "\n",
    "Research with this dataset is ongoing. Notably, many of these networks are quite large and quite expensive to train. \n",
    "\n",
    "## Citations\n",
    "\n",
    "Dataset: This tech report (Chapter 3) describes the dataset and the methodology followed when collecting it in much greater detail. Please cite it if you intend to use this dataset.\n",
    "[Learning Multiple Layers of Features from Tiny Images](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf), Alex Krizhevsky, 2009.\n",
    "\n",
    "[Normalization values for transforms:](https://stackoverflow.com/questions/66678052/how-to-calculate-the-mean-and-the-std-of-cifar10-data)\n",
    "\n",
    "[Transfer Learning for Computer Vision Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) - was referred by a very nice technical mentor I reached out to after going crazy all weekend with a feed-forward model that would not learn! :)\n",
    "\n",
    "\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell contains the essential imports you will need – DO NOT CHANGE THE CONTENTS! ##\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My additional imports and magics & stuff\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "import tarfile\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Specify your transforms as a list first.\n",
    "The transforms module is already loaded as `transforms`.\n",
    "\n",
    "CIFAR-10 is fortunately included in the torchvision module.\n",
    "Then, you can create your dataset using the `CIFAR10` object from `torchvision.datasets` ([the documentation is available here](https://pytorch.org/docs/stable/torchvision/datasets.html#cifar)).\n",
    "Make sure to specify `download=True`! \n",
    "\n",
    "Once your dataset is created, you'll also need to define a `DataLoader` from the `torch.utils.data` module for both the train and the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I'm struggling to put this all together, let's start with images themselves and remove the complexity of using these batches and pytorch objects, as they are just one more thing that can go wrong and aren't necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./cifar10.tgz\n"
     ]
    }
   ],
   "source": [
    "# Dowload the dataset\n",
    "dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\n",
    "download_url(dataset_url, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data from tar file:\n",
    "with tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n",
    "    tar.extractall(path='./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'test', 'train']\n",
      "['cat', '.DS_Store', 'dog', 'truck', 'bird', 'airplane', 'ship', 'frog', 'horse', 'deer', 'automobile']\n"
     ]
    }
   ],
   "source": [
    "# verifying the data is there:\n",
    "\n",
    "data_dir = './data/cifar10'\n",
    "\n",
    "print(os.listdir(data_dir))\n",
    "classes_list = os.listdir(data_dir + \"/train\")\n",
    "print(classes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of test examples for ship: 1000\n",
      "['0298.png', '0267.png', '0501.png', '0515.png', '0273.png']\n"
     ]
    }
   ],
   "source": [
    "ship_test_files = os.listdir(data_dir + \"/test/ship\")\n",
    "print(\"No. of test examples for ship:\", len(ship_test_files))\n",
    "print(ship_test_files[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(data_dir+'/train', transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element of this dataset contains an image tensor and a label. The images are 3-channels with dimensions of 32x32, so the expected tensor shape for each image will 3, 32, 32. The labels should just be a 1-dimensional tensor, but let's pick out an element to check that this is what we really have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32]) 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4431, 0.4314, 0.4353,  ..., 0.6039, 0.6431, 0.6745],\n",
       "         [0.4471, 0.4314, 0.4353,  ..., 0.6667, 0.6235, 0.6314],\n",
       "         [0.4588, 0.4549, 0.4471,  ..., 0.4902, 0.4471, 0.5294],\n",
       "         ...,\n",
       "         [0.2039, 0.2039, 0.2000,  ..., 0.6902, 0.6745, 0.6784],\n",
       "         [0.1765, 0.1804, 0.2000,  ..., 0.6745, 0.6784, 0.6863],\n",
       "         [0.1882, 0.2000, 0.2235,  ..., 0.6784, 0.6745, 0.6863]],\n",
       "\n",
       "        [[0.3843, 0.3725, 0.3725,  ..., 0.5961, 0.6431, 0.6745],\n",
       "         [0.3804, 0.3686, 0.3725,  ..., 0.6549, 0.6196, 0.6275],\n",
       "         [0.3765, 0.3804, 0.3765,  ..., 0.4784, 0.4392, 0.5216],\n",
       "         ...,\n",
       "         [0.2039, 0.1961, 0.1843,  ..., 0.6627, 0.6549, 0.6588],\n",
       "         [0.1882, 0.1804, 0.1961,  ..., 0.6510, 0.6549, 0.6667],\n",
       "         [0.2039, 0.2039, 0.2235,  ..., 0.6588, 0.6510, 0.6667]],\n",
       "\n",
       "        [[0.2706, 0.2588, 0.2627,  ..., 0.6078, 0.6510, 0.6824],\n",
       "         [0.2706, 0.2627, 0.2706,  ..., 0.6667, 0.6314, 0.6392],\n",
       "         [0.2588, 0.2745, 0.2706,  ..., 0.4902, 0.4510, 0.5333],\n",
       "         ...,\n",
       "         [0.1922, 0.2039, 0.2078,  ..., 0.6941, 0.6863, 0.6902],\n",
       "         [0.1843, 0.1843, 0.2078,  ..., 0.6784, 0.6863, 0.6941],\n",
       "         [0.2000, 0.2039, 0.2235,  ..., 0.6863, 0.6784, 0.6902]]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = dataset[7]\n",
    "print(img.shape, label)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.classes) #this is a property of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Dataset\n",
    "Using matplotlib, numpy, and torch, explore the dimensions of your data.\n",
    "\n",
    "You can view images using the `show5` function defined below – it takes a data loader as an argument.\n",
    "Remember that normalized images will look really weird to you! You may want to try changing your transforms to view images.\n",
    "Typically using no transforms other than `toTensor()` works well for viewing – but not as well for training your network.\n",
    "If `show5` doesn't work, go back and check your code for creating your data loaders and your training/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to change the image tensor to (32, 32, 3) for viewing and also adds the label and displayes the image.\n",
    "def show_example(img, label):\n",
    "    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n",
    "    plt.imshow(img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  automobile (1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0EAAAM6CAYAAABHGEjbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAB7CAAAewgFu0HU+AABUJklEQVR4nO39e5RcZZ0vcP+q70l3bpALJGkIBEKCysCBRBCZEAVdXCQmHNBZHCQcFHSUgXnFYS4OnlmjvsgRYV7OWjIRMF5mcDQCKjiIzmhQDAYkM6IEITfMDZIOIbfuTl+q3j+Y1CQk6SSknvTl+Xyyeq2drl3ffrp2PVX17b1rV6FUKpUCAAAgE1W9PQAAAIDDSQkCAACyogQBAABZUYIAAICsKEEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGRFCQIAALKiBAEAAFlRggAAgKwoQQAAQFaUIAAAICtKEAAAkBUlCAAAyEpNbw+gP2hvb49nn302IiJGjRoVNTVuNgAAOBy6urpiw4YNERHxtre9LRoaGg4506v5A/Dss8/GtGnTensYAACQtUWLFsXUqVMPOcfhcAAAQFbsCToAo0aNKi/PvXlWjBw2uOI/Y2TTyIpn7tTe1pose8eOrmTZTy57Lll2RMSabRuSZZ8w+ohk2X98/P9Ilt1U15gsu1Cd8G8uNemySwkPf62uOvTd+T2pqqlOll2oSneb1yQcd6lUSJbd2dmeLDuimCy5tq7yz2k7VVXXJcuOiCiWSsmyqyLdfaW7c0ey7LYtG5NlN44Znyw7qmqTRReLncmyS5HuPhgRUVVI9xxUiHTzs9JT8+WWV2PGtTdHxO6vyw+FEnQAdn0P0Mhhg2P0iKaK/4zRQ4dWPHOn9tZ0Lyja29M9sAxvqk+WHRGxuZTuAXfEkHRjP/qIIcmyh9RX/r69U6E63f0waQmqTXc/SV2CqhMWuJSlNuX7LtOWoLZk2VHqThZdm3DeV9UkLkHFhCWokLAEdaQrQa216f442TT6yGTZStDepS1B6V6rJPz7RMWeI/rd4XB/+MMf4qabboopU6ZEY2NjHHHEETFt2rT44he/GK2t6fZ4AAAAA0O/2hP0yCOPxBVXXBGbN28uf6+1tTWeeuqpeOqpp+Kee+6JH/7wh3H88cf34igBAIC+rN/sCfrP//zPuPzyy2Pz5s3R1NQUn/vc5+KXv/xl/Nu//Vt85CMfiYiI3//+93HRRRfFtm3benm0AABAX9Vv9gTdeOON0draGjU1NfHYY4/FWWedVb7sXe96V5x44onxF3/xF/H888/Hl770pbjlllt6cbQAAEBf1S/2BD311FPxs5/9LCIirrnmmt0K0E6f/OQnY8qUKRERceedd0ZnZ7o3wQEAAP1XvyhBDz30UHn56quv3us6VVVV8aEPfSgiIjZt2lQuTQAAALvqFyXo5z//eURENDY2xumnn77P9aZPn15e/sUvfpF8XAAAQP/TL94TtGTJkoiIOOGEE3o8N/jkyZP3uM6BWL16dY+Xr1u37oCzAACAvq3Pl6D29vZoaWmJiIjx43v+pOIRI0ZEY2NjbN++PVatWnXAP6O5ufmQxggAAPQfff5wuK1bt5aXm5r2/6nWjY2NERFOkw0AAOxVv9gTtFNdXd1+16+vr4+IiLa2tgP+Gfvba7Ru3bqYNm3aAecBAAB9V58vQQ0NDeXljo6O/a6/Y8eOiIgYNGjQAf+M/R1mBwAADBx9/nC4IUOGlJcP5BC37du3R8SBHToHAADkp8+XoIaGhhg5cmRE7P8sbps2bSqXICc7AAAA9qbPl6CIiClTpkRExNKlS6Orq2uf6z3//PN7XAcAAGBX/aIEvfOd74yI1w91+/Wvf73P9RYsWFBePvvss5OPCwAA6H/6RQl6//vfX17+6le/utd1isVifP3rX4+IiOHDh8eMGTMOx9AAAIB+pl+UoGnTpsU555wTERH33ntvLFy4cI91br/99liyZElERNxwww1RW1t7WMcIAAD0D33+FNk7/cM//EOcffbZ0dbWFu95z3vir//6r2PGjBnR1tYW3/rWt2Lu3LkRETFp0qT45Cc/2cujBQAA+qp+U4JOO+20+Jd/+Zf4X//rf8WWLVvir//6r/dYZ9KkSfHII4/sdlptAACAXfWbEhQR8b73vS9+85vfxD/8wz/EI488EqtXr466uro44YQT4rLLLotPfOITMXjw4KRjqK8dFA11lf8ZpVK6TTGoqmH/K71JQwalO+xwWENjsuyIiLqtpWTZXd37PovhoaqqSncUa6lYTJcd6W7vQnW6+2GxuztZdnUxXXZERCHhEc+lQrLoKBbT3VdSSniTRDHhY0p3wvthIeG8f/0HJHzMSvi8XCqmu7fU1aZ7zk96L0/4oFJKdzeJQlXKmR9RKnUmy074lB9VFX/tWfnbuV+VoIiIY489Nr70pS/Fl770pd4eCgAA0A/1ixMjAAAAVIoSBAAAZEUJAgAAsqIEAQAAWVGCAACArChBAABAVpQgAAAgK0oQAACQFSUIAADIihIEAABkRQkCAACyogQBAABZUYIAAICsKEEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGRFCQIAALKiBAEAAFlRggAAgKzU9PYA+puqqtqoqqqteG5dTV3FM3cqtrcmy45Sd7LoEfXpbpOIiCPrBiXLrk7454VSVSldeLEzXXapmCy6EOnuK4WudPfxrqr2ZNkREVVVhWTZhWLlHwd3KkV1suxCIeFtknDcKad9XXW6lwLd3QkHHhHVhYT3leqEz0G16R5XuovpbpNiV1uy7ELC1xPFhI/jhZq0+xMSPoxHKeHjYVeFn/MrnRdhTxAAAJAZJQgAAMiKEgQAAGRFCQIAALKiBAEAAFlRggAAgKwoQQAAQFaUIAAAICtKEAAAkBUlCAAAyIoSBAAAZEUJAgAAsqIEAQAAWVGCAACArChBAABAVpQgAAAgK0oQAACQFSUIAADIihIEAABkRQkCAACyogQBAABZUYIAAICs1PT2APqbzq7O6OzsrHhu9eDqimeWs6trk2WXujqSZY8ZOihZdkREe/eRybLrEs6sqs50t3mpppAsOwrp/ubS1d6aLLtQk25u1tQ1JcuOiCiWiunCu9Nld3V2Jcvu7OpOlr10dUuy7I6udLf3iccOSZY96sjBybIjIrrTbc6oLqV7PCx27UiXnfBGKXZX/vXPToVSunkfCR8KS6V0r7EiIkqFdK+FSlV1ybIjKj1/Kv8awp4gAAAgK0oQAACQFSUIAADIihIEAABkRQkCAACyogQBAABZUYIAAICsKEEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGRFCQIAALKiBAEAAFlRggAAgKwoQQAAQFaUIAAAICtKEAAAkBUlCAAAyIoSBAAAZEUJAgAAsqIEAQAAWanp7QH0NzXV1VFTU13x3EIpXR/tKlZ+vDuVOtON+6ghRyXLjojo6ioly64qtqfL3r4jWXaxupgsu9AwKFl2MTqSZRe6E/6tqLYuXXZEFKrTzf2IdPeVrVs3J8te+PRvkmX/5uW2ZNm1Q4Ymy37u5VeSZU9sHpssOyJi7JhxybK3bEn3uFIVncmyxwwfliy7qT3dY0pTQ7rH2uradNml6tpk2RER3YV0L9VLVem2Z8Vv8arKb0N7ggAAgKwoQQAAQFaUIAAAICtKEAAAkBUlCAAAyIoSBAAAZEUJAgAAsqIEAQAAWVGCAACArChBAABAVpQgAAAgK0oQAACQFSUIAADIihIEAABkRQkCAACyogQBAABZUYIAAICsKEEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGRFCQIAALJS09sD6G+GDRoeIwYPq3huMQoVzyyrq0sWXSgNSpZdvaMtWXZExJDqdHf/usbRybKrBw1Pll0odCfLrq6tTZZd6k43f6qr0v2tqH1HZ7LsiIhVr+1Ilj1s9LHJso8YMjRZ9sjmE5JlD+tanSx79YZNybKPHD0yWfbGdHfBiIhY+evfJ8suVTcmy64dVvnXETtta0g3f4Z3lZJlD25L95w/+oh0r1UG1aR7bouI6C6mu80TvvKMYqGy6aUK50XYEwQAAGSmX5SgQqFwQF/nnntubw8VAADo4/pFCQIAAKiUfvWeoI997GPxp3/6p/u8vLEx3fG7AADAwNCvStDo0aPjrW99a28PAwAA6MccDgcAAGRFCQIAALKiBAEAAFnpV+8J+s53vhP3339//OEPf4iampo46qij4h3veEfMmTMnZsyY8aZzV6/u+QPu1q1b96azAQCAvqVflaDnnntut/8vXbo0li5dGl//+tfj/e9/f8ybNy+GvYlPYW5ubq7UEAEAgD6uX5SgwYMHxyWXXBLvfve7Y/LkydHU1BQbNmyIBQsWxN133x0bN26Mhx56KGbOnBk//vGPo7a2treHDAAA9FH9ogStWbMmhg8fvsf3zz///Lj++uvjggsuiMWLF8eCBQviy1/+cvzZn/3ZQeWvWrWqx8vXrVsX06ZNO6hMAACgb+oXJWhvBWinMWPGxPz582PKlCnR0dERd91110GXoPHjxx/iCAEAgP5iQJwd7vjjj4/zzz8/Il5/n9DatWt7eUQAAEBfNSBKUETEySefXF5es2ZNL44EAADoywZMCSqVSr09BAAAoB8YMCVo19Nnjx07thdHAgAA9GUDogQtX748fvzjH0fE6+8PGjduXC+PCAAA6Kv6fAn6wQ9+EF1dXfu8/JVXXon/+T//Z3R2dkZExMc//vHDNTQAAKAf6vOnyL7++uujs7MzLr300jjrrLNiwoQJMWjQoGhpaYmf/exn5Q9LjYh45zvfqQQBAAA96vMlKCJi7dq1cdddd8Vdd921z3UuvfTSuOeee6K+vv4wjgwAAOhv+nwJ+trXvhYLFiyIhQsXxvLly6OlpSW2bNkSTU1N0dzcHO94xzviqquuirPOOqu3hwoAAPQDfb4ETZ8+PaZPn97bwyirqxkc9TVNFc/t7OF9T4cq5cnDSynTC2lPez6ktiFZdu3g4cmyS41DkmUXip3JsktV1cmyqxI+kjXUpwtfunp9suyIiBfWtyfLPmbwscmytzcclSz7yMlHJ8setOG1ZNnrn30xWfbajduSZb/33Wmfv0ePG5Mse/PGV9Nlb345WfaCl15Klv2+C96XLLu10J0se1nLxmTZE0ane26LiBhUly4/5afLVPqja1J8FE6fPzECAABAJSlBAABAVpQgAAAgK0oQAACQFSUIAADIihIEAABkRQkCAACyogQBAABZUYIAAICsKEEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGRFCQIAALKiBAEAAFlRggAAgKwoQQAAQFaUIAAAICtKEAAAkBUlCAAAyIoSBAAAZKWmtwfQ3xSiFIUoVjy3VOyueOZOxWLlx7tTIWV2oZAsOyKirqExWXZ3upslopTudqkeNDhddoJ5s1NVpLtNNrW2J8v++e9WJMuOiHjL2/84WfbkUyYly/71L59Klr2pLd3f/rbtSPeYMm7sUcmyX1q/Pln2I//6o2TZEREf+sClybLrhjQlyx68fXuy7NHDS8myN2xYlyz7j047M1n2stZ0r7F+uWhxsuyIiHe94+R04VVpX2dVVILXhPYEAQAAWVGCAACArChBAABAVpQgAAAgK0oQAACQFSUIAADIihIEAABkRQkCAACyogQBAABZUYIAAICsKEEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGRFCQIAALKiBAEAAFlRggAAgKwoQQAAQFaUIAAAICtKEAAAkBUlCAAAyEpNbw+gvykWaqJYVVv53OiqeOZOhUIhWXaxO924a2sbkmVHRNQ2HZksu1jVnSw7Sulu87aO6mTZm9vT3SatUZcse+jRb02W3XDExmTZEREdHTuSZe9ofS1ZdvWODcmyFz3+dLLs5j86P1n2iSdOTZb96rafJcve+tqmZNkREf/2s8eTZTc2DUuWve7ldPfxY8aNSZY9aPDyZNnDjhiXLPt3L6xIlv37X6V7TImIOOOUE5JlDxs5JFl2qatY2cCqyr8+sScIAADIihIEAABkRQkCAACyogQBAABZUYIAAICsKEEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGRFCQIAALKiBAEAAFlRggAAgKwoQQAAQFaUIAAAICtKEAAAkBUlCAAAyIoSBAAAZEUJAgAAsqIEAQAAWVGCAACArChBAABAVmp6ewD9TUepO3YUuyueW6gqVDyznJ0uOtqL6bIHNR2ZLjwiqgYNSZZdm3B7rnhlTbLsJ19ckSx7+SvbkmWf9NZTk2XPnHZ8suwjBi9Klh0R8dTCXyXLrm5Ptz07SvXJsjd0lJJlr33658myz3n7Rcmyu4ojkmVv3LoqWXZERKm0Lll2TdX6ZNkvrnk1WfYLK/6QLPukl9cmy37hD5uTZS9e/B/JsuurO5JlR0T8x/J0c+iPj3xLsuyqQnVF8wpR2bwIe4IAAIDMKEEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGRFCQIAALKiBAEAAFlRggAAgKwoQQAAQFaUIAAAICtKEAAAkBUlCAAAyIoSBAAAZEUJAgAAsqIEAQAAWVGCAACArChBAABAVpQgAAAgK0oQAACQFSUIAADIihIEAABkpaa3B9DfFNu7otjWWfncUrHimTtVFauTZVcPHp4su7OmPll2REQhupNlr9+yI1n2d3+1NFn2H17dmiy7rj7d9jzm2KOTZXe3v5Yse1tHa7LsiIjfvrQqWfbazenG/r8+8MFk2f+fG/4oWfb6NcuTZW9sKyTL3rY93fPPls1tybIjIo45enSy7OOOGpUsu66uIVn25h3ptuerm7Yny16z7hfJsgc1jkiWfdyE05JlR0T8cMGvkmWPGjooWfbJJxxX4cRShfPsCQIAADKjBAEAAFlJWoLWr18fDz/8cNxyyy1xwQUXxMiRI6NQKEShUIg5c+YcdN6jjz4as2fPjvHjx0d9fX2MHz8+Zs+eHY8++mjlBw8AAAxISd8TNGbMmIrklEql+OhHPxpz587d7ftr1qyJBx98MB588MG49tpr4+67745CId1x0wAAQP932A6Ha25ujve85z1v6rqf/vSnywXotNNOi/vvvz8WLVoU999/f5x22utvSJs7d2787d/+bcXGCwAADExJ9wTdcsstMXXq1Jg6dWqMGTMmVq5cGccdd3Bni1i6dGncdtttERFxxhlnxOOPPx6DBr1+NoupU6fGJZdcEtOnT4+nn346vvCFL8TVV18dEydOrPjvAgAADAxJ9wT93d/9XVx88cWHdFjcHXfcEV1dXRERcdddd5UL0E6DBw+Ou+66KyIiurq64s4773zTPwsAABj4+vTZ4UqlUnzve9+LiIjJkyfHmWeeudf1zjzzzDjppJMiIuKhhx6KUqny5xIHAAAGhj5dglasWBFr1qyJiIjp06f3uO7Oy1evXh0rV65MPTQAAKCfSvqeoEO1ZMmS8vLkyZN7XHfXy5csWXJQ7z1avXp1j5evW7fugLMAAIC+rU+XoFWrVpWXx48f3+O6zc3Ne73egdj1ugAAwMDWpw+H27p1a3m5qampx3UbGxvLy9u2bUs2JgAAoH/r03uC2tvby8t1dXU9rltfX19ebmtrO6ifs789R+vWrYtp06YdVCYAANA39ekS1NDQUF7u6Ojocd0dO3aUl994Gu392d+hdgAAwMDRpw+HGzJkSHl5f4e4bd++vby8v0PnAACAfPXpErTrHpr9ncFt10PanOgAAADYlz5dgk4++eTy8vPPP9/jurtePmXKlGRjAgAA+rc+XYKOO+64GDt2bERELFiwoMd1H3/88YiIGDduXEyYMCH10AAAgH6qT5egQqEQM2fOjIjX9/Q8+eSTe13vySefLO8JmjlzZhQKhcM2RgAAoH/p0yUoIuLGG2+MmprXT2J3/fXX73H667a2trj++usjIqKmpiZuvPHGwz1EAACgH0l6iuxf/OIXsXTp0vL/W1paystLly6NefPm7bb+nDlz9siYNGlS3HTTTXHrrbfG008/HWeffXbcfPPNMXHixFi2bFl84QtfiMWLF0dExKc+9ak48cQTk/wuAADAwJC0BN1zzz3xta99ba+XPfHEE/HEE0/s9r29laCIiM997nOxfv36uO+++2Lx4sXxwQ9+cI91rrnmmvjsZz97yGMGAAAGtj79Yak7VVVVxb333huXXnppzJ07N5566qloaWmJkSNHxtSpU+O6666LCy644LCMpRSFKEbl33PUUUq3KeoaGtNl16Y7orKqupgsOyKiu9jzB/Aeit8s35As+7fLej5d/KEYOjTdZ2zVVae7r/zuxWXJsk868aRk2aPGH5csOyJix46eTyhzKH67dHmy7F8u+mWy7Gs/eFmy7HGD0n08w/OrXkuWXVVbnSy7ulBKlh2x+2cEVlrLloP74PWDccopb02W/fKmnj9X8VC8tGJtsuz2/Xwe5KE46sixybJfXZ/uNomIeOGF3yTLPmPS+P2v9CaddNwxFc3r7q78a8KkJWjevHl7HPJ2KC688MK48MILK5YHAADkp8+fGAEAAKCSlCAAACArShAAAJAVJQgAAMiKEgQAAGRFCQIAALKiBAEAAFlRggAAgKwoQQAAQFaUIAAAICtKEAAAkBUlCAAAyIoSBAAAZEUJAgAAsqIEAQAAWVGCAACArChBAABAVpQgAAAgK0oQAACQFSUIAADIihIEAABkpaa3B9Df1NTVRW19fcVzC1Fd8cxydnXCzVzsTBZdW11Ilh0RsXLdq8myf7poSbLs+vrGZNlbNm9Lll1d05As+9n/fC5ZdrG1K1n2af/jlGTZERFnnzUjWfa/P/HzdNk//2Wy7EkTT0iWXRXdybJfe+XlZNnDjxiSLPvkKRcmy46IaHl5Q7LsmoZ0t8vKtemef158/vlk2Zu3bkmWPeKIMcmyW15enSy7GNuTZUdEHDN+ZLLs0UeMSpYdxQq/ri1Wfr+NPUEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGRFCQIAALKiBAEAAFlRggAAgKwoQQAAQFaUIAAAICtKEAAAkBUlCAAAyIoSBAAAZEUJAgAAsqIEAQAAWVGCAACArChBAABAVpQgAAAgK0oQAACQFSUIAADIihIEAABkRQkCAACyogQBAABZqentAfQ3NdU1UVOT4GbrKlY+878UuruSZXeX0o27VEyXHRHxH8/9Pln2k7/9VbLso5tPTJY9ZPCQZNktGzYmyx42ckSy7OeXrkiW3VlKFh0REWe8/exk2duL6R5X/vXRHyXLfvBHP0uWXVvdmSx79MgxybJHjhibLHva1GnJsiMiFi58Jln2kt89lyx7w7pVybI7Wrcmy66qS/eycevm15Jljzgq3XPE295ySrLsiIjN2zcnyx49alSy7Eq/hEvxktCeIAAAICtKEAAAkBUlCAAAyIoSBAAAZEUJAgAAsqIEAQAAWVGCAACArChBAABAVpQgAAAgK0oQAACQFSUIAADIihIEAABkRQkCAACyogQBAABZUYIAAICsKEEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGRFCQIAALKiBAEAAFmp6e0B9DeFqkJUVRUqn1soVTxzp2KxO1l2ISp/W+zUtqU1WXZExK9/tyRZ9rb2HcmyV69eliz7hGMnJ8tuaByRLHv1uleSZR8xYmiy7B3PtSfLjohYvWZNsuyGwQ3Jsk86aUqy7K3btyXLnjz55GTZJxx3YrLs53/7QrLs7/7LD5NlR0Sse3lVsuxXW9LNnyh2psuuSvd6IorpsrvatyfLfsvktyfLPnnSScmyIyLWvrI2WXZdXV2y7Eq/9iyWihXNi7AnCAAAyIwSBAAAZEUJAgAAsqIEAQAAWVGCAACArChBAABAVpQgAAAgK0oQAACQFSUIAADIihIEAABkRQkCAACyogQBAABZUYIAAICsKEEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGRFCQIAALKiBAEAAFlRggAAgKwoQQAAQFZqensA/U1nR0d07OiofG5XV8UzdyqVSsmyB9fVJ8tesmxVsuyIiBf+0JIsu1BI9/eF7o4dybLXvLQ0WfbIoycmyx4xfFSy7C1bN6XLLm1Nlh0R0bIl3dhLXcVk2dXV6Z6aBg8aniw7ioOSRT/325XJspe+sCJZ9sgjG5NlR0R0d25JGJ7usbYm4d+gq6rrkmV3FNPN+9GjjkyWfeTRzcmytxTTvcaKiBh5xMhk2cMam5Jllyp8u1Q6L8KeIAAAIDNJS9D69evj4YcfjltuuSUuuOCCGDlyZBQKhSgUCjFnzpwDypg3b175Ovv7mjdvXspfBwAAGACSHg43ZsyYlPEAAAAH7bC9J6i5uTmmTJkSjz322JvO+NGPfhRjx47d5+Xjx49/09kAAEAekpagW265JaZOnRpTp06NMWPGxMqVK+O4445703mTJk2KCRMmVG6AAABAdpKWoL/7u79LGQ8AAHDQnB0OAADIihIEAABkpV99WOqcOXNiyZIlsWnTphg6dGiccMIJcd5558XHPvaxGDdu3JvOXb16dY+Xr1u37k1nAwAAfUu/KkELFiwoL2/cuDE2btwYv/rVr+L222+PO++8M6677ro3ldvcnO6ThAEAgL6lX5Sg448/PmbPnh1nnXVWubAsX748vvvd78b8+fOjvb09PvrRj0ahUIhrr722l0cLAAD0ZX2+BM2aNSuuuuqqKBQKu31/6tSp8YEPfCAefvjhmD17dnR2dsaf//mfxyWXXBJHHXXUQf2MVatW9Xj5unXrYtq0aQc9dgAAoO/p8ydGGDZs2B4FaFcXX3xxfOYzn4mIiNbW1rj33nsP+meMHz++x6+jjz76TY8fAADoW/p8CToQH/nIR8pFadf3DQEAALzRgChBo0ePjpEjR0ZExJo1a3p5NAAAQF82IEpQRESpVOrtIQAAAP3AgChB69evj40bN0ZExNixY3t5NAAAQF82IErQ3Llzy3uCpk+f3sujAQAA+rI+XYJWrlwZixcv7nGdhx9+OP7+7/8+IiIaGhri6quvPhxDAwAA+qmknxP0i1/8IpYuXVr+f0tLS3l56dKlMW/evN3WnzNnzm7/X7lyZcyYMSPOOuuseN/73hennnpqjB49OkqlUixfvjzmz58f8+fPL+8F+uIXvxjjxo1L9vsAAAD9X9ISdM8998TXvva1vV72xBNPxBNPPLHb995YgnZauHBhLFy4cJ8/Z/DgwXHHHXfEtdde+6bHCgAA5CFpCTpUp59+enzzm9+MhQsXxtNPPx3r1q2LlpaW6OrqihEjRsRb3vKWePe73x0f/vCHY/To0b09XAAAoB9IWoLmzZu3xyFvB2PIkCFxxRVXxBVXXFG5QR2irs4d0dnRXvngQrq3Z1VXJXzrV8Jxr+qoTpYdEfFqZ7rTqldHuuyG+qZk2Z0dncmy/7BiSbLsEUelOyvk2LHHJMtu39GdLDsioq1te7Ls2praZNnVVememtra0t3Hf7/kuWTZbdvTbcskz2n/pW37K8myIyJ2tL6aLLumkO5xvFAzJFl2qZBu/tQWtiXLrqspJMvu6upKlh3t6W6TiIghteleZ9VWp7vNO4uVfX7rqnBeRB8/MQIAAEClKUEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGRFCQIAALKiBAEAAFlRggAAgKwoQQAAQFaUIAAAICtKEAAAkBUlCAAAyIoSBAAAZEUJAgAAsqIEAQAAWVGCAACArChBAABAVpQgAAAgK0oQAACQFSUIAADIihIEAABkpaa3B9DfFP7rq+KqqlOkRkRETW1tsuzXtu9Ilr1ux+Bk2RERo8e/NVn2shd/nSy76YjGZNk1w9I9JKxatz5Z9rpVf0iWvXnjxmTZzcf8UbLsiIiRI45Klv3qa2uSZe9oLyXLLkVXsuzo6EwWXexqS5bdsWN7suyutnTZERHV1emeO+ubhifLrq4dlCw72rcmix5Tl+5v5xNqWpNlD/nD08myayPdvI+IKIw7Pll2V9XbkmVHobKvljsLlZ/r9gQBAABZUYIAAICsKEEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGRFCQIAALKiBAEAAFlRggAAgKwoQQAAQFaUIAAAICtKEAAAkBUlCAAAyIoSBAAAZEUJAgAAsqIEAQAAWVGCAACArChBAABAVpQgAAAgK0oQAACQFSUIAADISk1vD6C/KRZqo1hVV/HcUlRXPPO/pdvMNbXJomPDH36fLjwijj325GTZW7e9liz7lY0rk2UPbmxKll1MlhxRU51u/mxv3Z4se0PLmmTZERFdpSOSZTcNGp0su/mEk5Jlb9qyNln2H5Y9myy7o31LsuxSV2ey7IbBQ5JlR0Q0DR2ZLLujc0ey7EGdG5NlH9uYLDomj0y3PY8ble7558jh6V4HVdemG3dERGdDurFXFduTZVdX+LVyTXRXNC/CniAAACAzShAAAJAVJQgAAMiKEgQAAGRFCQIAALKiBAEAAFlRggAAgKwoQQAAQFaUIAAAICtKEAAAkBUlCAAAyIoSBAAAZEUJAgAAsqIEAQAAWVGCAACArChBAABAVpQgAAAgK0oQAACQFSUIAADIihIEAABkRQkCAACyogQBAABZqentAfQ3VbX1UVVbX/HcYrHikf+d3V1Kll1Xm+4uNGX80GTZERE/fm5psuxpbz8vWfbPn/hhsuy1a19Ill1dNThZdqE63f2wppDub0WFUiFZdkTEmCOPS5Y9ZfLpybKXLn82XfbS3yTLbt2+Pll2fVW6+8rQI8Ymy64qpH2ZUdW1PVn2sM7NybInDUt3u7xldGOy7FHD02UPHdaULHvQ4EHJsru7E76Ai4iGI0cmyy51tCbL3vHahormdby6qaJ5EfYEAQAAmVGCAACArChBAABAVpQgAAAgK0oQAACQFSUIAADIihIEAABkRQkCAACyogQBAABZUYIAAICsKEEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGRFCQIAALKiBAEAAFlRggAAgKwoQQAAQFaUIAAAICtKEAAAkBUlCAAAyEpNbw+g36mqikJVdcVjC6XuimfuVOouJsve1tmZLPstJ/9RsuyIiJ/85oFk2S8t/32y7DOnnpcs+5cLO5Jlv/Ly2mTZXaVSsuzuqnR/Kzrp2MZk2RERl04dmSz74Wd+kSz7hWW/S5bdNCjd0962Lekex4s1lX/e2ammJt386dr6SrLsiIgjC63JsqeMHJYse+LIwcmyjxyc7r5SW5tu/hSqC8myi9GVLLtu8KBk2RERjaOOTpZdqKpNlt352suVzduypaJ5EfYEAQAAmVGCAACArCQtQc8880x8/vOfjwsuuCCam5ujvr4+mpqaYtKkSTFnzpz4+c9/flB5jz76aMyePTvGjx8f9fX1MX78+Jg9e3Y8+uijiX4DAABgoEl2cOf06dPj8ccf3+P7HR0d8eKLL8aLL74YX/va1+LKK6+Me+65J+rq6vaZVSqV4qMf/WjMnTt3t++vWbMmHnzwwXjwwQfj2muvjbvvvjsKhXTHlAIAAP1fsj1Ba9asiYiIsWPHxg033BDz58+PRYsWxcKFC+NLX/pSjBs3LiIivvGNb8ScOXN6zPr0pz9dLkCnnXZa3H///bFo0aK4//7747TTTouIiLlz58bf/u3fpvp1AACAASLZnqDJkyfH5z//+bj00kujunr3M5WceeaZceWVV8bZZ58dL7zwQtx///3xsY99LM4555w9cpYuXRq33XZbREScccYZ8fjjj8egQa+fiWPq1KlxySWXxPTp0+Ppp5+OL3zhC3H11VfHxIkTU/1aAABAP5dsT9DDDz8cl19++R4FaKeRI0fG7bffXv7//Pnz97reHXfcEV1dr5/a8K677ioXoJ0GDx4cd911V0REdHV1xZ133lmB0QMAAANVr54d7txzzy0vL1u2bI/LS6VSfO9734uI1/csnXnmmXvNOfPMM+Okk06KiIiHHnooSgk/MwQAAOjferUEdXT89wczVu3lAwlXrFhRfm/R9OnTe8zaefnq1atj5cqVlRskAAAwoKT76N8DsGDBgvLy5MmT97h8yZIlPV6+q10vX7JkSRx33HEHPI7Vq1f3ePm6desOOAsAAOjbeq0EFYvFuPXWW8v/v/zyy/dYZ9WqVeXl8ePH95jX3Ny81+sdiF2vCwAADGy9djjcHXfcEYsWLYqIiFmzZsUZZ5yxxzpbt24tLzc1NfWY19jYWF7etm1bhUYJAAAMNL2yJ2jBggXxl3/5lxERMXr06Pjyl7+81/Xa29vLyz19mGpERH19fXm5ra3toMazvz1H69ati2nTph1UJgAA0Dcd9hL0u9/9LmbNmhVdXV1RX18f3/72t2PMmDF7XbehoaG8vOtJFPZmx44d5eU3nkZ7f/Z3qB0AADBwHNbD4VasWBHvec97YtOmTVFdXR33339/j2d9GzJkSHl5f4e4bd++vby8v0PnAACAfB22ErR27do477zzYu3atVEoFOK+++6LWbNm9XidXffQ7O8Mbrse0uZEBwAAwL4clhLU0tIS559/fixfvjwiIu6666740Ic+tN/rnXzyyeXl559/vsd1d718ypQpb3KkAADAQJe8BG3evDne+973xnPPPRcREbfeemt8/OMfP6DrHnfccTF27NiI2P0zhfbm8ccfj4iIcePGxYQJE978gAEAgAEtaQlqbW2Niy66KJ555pmIiPibv/mbuPnmmw/4+oVCIWbOnBkRr+/pefLJJ/e63pNPPlneEzRz5swoFAqHOHIAAGCgSlaCOjo6YtasWfHEE09ERMQNN9wQn/3sZw8658Ybb4yamtdPYnf99dfvcfrrtra2uP766yMioqamJm688cZDGzgAADCgJTtF9p/8yZ/EY489FhER73rXu+Kaa66J3/72t/tcv66uLiZNmrTH9ydNmhQ33XRT3HrrrfH000/H2WefHTfffHNMnDgxli1bFl/4whdi8eLFERHxqU99Kk488cQ0vxAAADAgJCtBDzzwQHn53//93+OUU07pcf1jjz02Vq5cudfLPve5z8X69evjvvvui8WLF8cHP/jBPda55ppr3tSeJgAAIC+H9XOC3qyqqqq4995745FHHomZM2fG2LFjo66uLsaOHRszZ86MH/7wh3HPPfdEVVW/+HUAAIBelGxPUKlUqnjmhRdeGBdeeGHFcw9KqfT6V4WlPJlDMVlyRFd3uuxidUO68Ig45pgJybK/99iPk2V3dPb8wcGH4n+cdk6y7GVLf5cs+6WXfp8su6Er3Qy64MiNybIjIja88G/Jsn/zbLqxT56y56HRlXLsuGOSZT/7u+pk2WtXL0uWPWhbz5/DdyjGNaa7TSIiJgxP9+Ho448YnCx7UH2626W6JmF2dbo/NpcS/l2+qpAuu3FUuseUiIjahFOo9dU1ybLbtm+taF57a+Vf+9h1AgAAZEUJAgAAsqIEAQAAWVGCAACArChBAABAVpQgAAAgK0oQAACQFSUIAADIihIEAABkRQkCAACyogQBAABZUYIAAICsKEEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGRFCQIAALKiBAEAAFlRggAAgKwoQQAAQFaUIAAAICs1vT2A/qZYLEV3sVj54FLlI3cqlhKM97+UCul69PbtW5JlR0RsWLcqYfqOZMnPPferZNlt7duTZZ/6R2cmyx41dmKy7NZlv0mW/dtFm5JlR0T8ov21ZNkNR49Olj311FOTZR8xdGiy7I2rliXLrm+oTZb9R6OHJMseP6QhWXZERG1dupcxNXV1ybILtenGXdVQnSy7OrqTZRfr091X6sYemyy7/sijkmVHRKR4yblTdXdnsuwhQ4dVNG97V0XjIsKeIAAAIDNKEAAAkBUlCAAAyIoSBAAAZEUJAgAAsqIEAQAAWVGCAACArChBAABAVpQgAAAgK0oQAACQFSUIAADIihIEAABkRQkCAACyogQBAABZUYIAAICsKEEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGRFCQIAALKiBAEAAFmp6e0B9DdVhUJUFQoVz+0qdlc8c6dSsuSImgS3xU6vrV2ZLDsiorR5XbLsIYNqk2V3bO9Mlv3y2t8nyx7cuT1Z9tCxJyfLHjJxSrLsH/3nomTZERGtpbZk2aeMGJose+SII5JlF7u6kmUfVdWeLPv45mHJspuHD06WPaimOll2RERXVbrH2qhK93fimup0L7/qBzUky24clm5u1o88Jln2oJFHJ8suFdLex2tq6pNld9U3Jcsudr9W0bxCofLz0Z4gAAAgK0oQAACQFSUIAADIihIEAABkRQkCAACyogQBAABZUYIAAICsKEEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGRFCQIAALKiBAEAAFlRggAAgKwoQQAAQFaUIAAAICtKEAAAkBUlCAAAyIoSBAAAZEUJAgAAsqIEAQAAWanp7QH0N6X/+lfx3FLlMw9HdveOHcmyC1s3JsuOiJg4pCFZdmfn0GTZS7q2JMve3NmZLHvN+hXJspetXZUsu2pwum1ZE4Vk2RER0d2dLLq+ri5ZdldNdbLs0tZNybKPqOpIlt3UlO7xqlBIdz+srq1Nlh0RUVufLr+zO91zZ3XC+3h907Bk2cOOOTlZ9qChI5Nll6rS3d472jYny46IKHZ3pQuvTlcDqmoqm13pvAh7ggAAgMwoQQAAQFaUIAAAICtKEAAAkBUlCAAAyIoSBAAAZEUJAgAAsqIEAQAAWVGCAACArChBAABAVpQgAAAgK0oQAACQFSUIAADIihIEAABkRQkCAACyogQBAABZUYIAAICsKEEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGSlprcH0N90d3VFd1dXxXNLpYpHlhWLxWTZpe7K3xY7DR6cLDoiIkY21SfLntiV7javjnR3lqWbtyXL3rQ93X0lSoVk0YX2dLdJVyHhxI+I7Z3tybKL1Qlv82JrsuytK/4zWXZ9dCTLrqupS5Zd39iYLLvxyBHJsiMihgwdkiy7bnBTsuyGoUOTZTcOPyJZdu3gdNuzqibd3+WrCumy6+rTbcuIiIQv4aJQqE6WXWqsbPbWBPtt7AkCAACykrQEPfPMM/H5z38+Lrjggmhubo76+vpoamqKSZMmxZw5c+LnP//5fjPmzZsXhULhgL7mzZuX8tcBAAAGgGSHw02fPj0ef/zxPb7f0dERL774Yrz44ovxta99La688sq45557oq4u3a5+AACAnZKVoDVr1kRExNixY+Oyyy6Lc845J4455pjo7u6OhQsXxu233x5r1qyJb3zjG9HV1RX//M//vN/MH/3oRzF27Nh9Xj5+/PiKjR8AABiYkpWgyZMnx+c///m49NJLo7p69zdHnXnmmXHllVfG2WefHS+88ELcf//98bGPfSzOOeecHjMnTZoUEyZMSDVkAAAgA8neE/Twww/H5ZdfvkcB2mnkyJFx++23l/8/f/78VEMBAAAo69Wzw5177rnl5WXLlvXeQAAAgGz0agnq6Pjvz1SoqnK2bgAAIL1e/bDUBQsWlJcnT5683/XnzJkTS5YsiU2bNsXQoUPjhBNOiPPOOy8+9rGPxbhx4970OFavXt3j5evWrXvT2QAAQN/SayWoWCzGrbfeWv7/5Zdfvt/r7FqaNm7cGBs3boxf/epXcfvtt8edd94Z11133ZsaS3Nz85u6HgAA0P/0Wgm64447YtGiRRERMWvWrDjjjDP2ue7xxx8fs2fPjrPOOqtcWJYvXx7f/e53Y/78+dHe3h4f/ehHo1AoxLXXXntYxg8AAPRPvVKCFixYEH/5l38ZERGjR4+OL3/5y/tcd9asWXHVVVdFoVDY7ftTp06ND3zgA/Hwww/H7Nmzo7OzM/78z/88LrnkkjjqqKMOajyrVq3q8fJ169bFtGnTDioTAADomw772Qh+97vfxaxZs6Krqyvq6+vj29/+dowZM2af6w8bNmyPArSriy++OD7zmc9ERERra2vce++9Bz2m8ePH9/h19NFHH3QmAADQNx3WErRixYp4z3veE5s2bYrq6uq4//77Y/r06Yec+5GPfKRclHZ93xAAAMAbHbYStHbt2jjvvPNi7dq1USgU4r777otZs2ZVJHv06NExcuTIiIhYs2ZNRTIBAICB6bCUoJaWljj//PNj+fLlERFx1113xYc+9KGK/oxSqVTRPAAAYGBKXoI2b94c733ve+O5556LiIhbb701Pv7xj1f0Z6xfvz42btwYERFjx46taDYAADCwJC1Bra2tcdFFF8UzzzwTERF/8zd/EzfffHPFf87cuXPLe4Iq8R4jAABg4EpWgjo6OmLWrFnxxBNPRETEDTfcEJ/97GcPKmPlypWxePHiHtd5+OGH4+///u8jIqKhoSGuvvrqNzdgAAAgC8k+J+hP/uRP4rHHHouIiHe9611xzTXXxG9/+9t9rl9XVxeTJk3a7XsrV66MGTNmxFlnnRXve9/74tRTT43Ro0dHqVSK5cuXx/z582P+/PnlvUBf/OIXY9y4cal+JQAAYABIVoIeeOCB8vK///u/xymnnNLj+scee2ysXLlyr5ctXLgwFi5cuM/rDh48OO6444649tpr39RYAQCAfCQrQZVw+umnxze/+c1YuHBhPP3007Fu3bpoaWmJrq6uGDFiRLzlLW+Jd7/73fHhD384Ro8e3dvDBQAA+oFkJagSp6weMmRIXHHFFXHFFVdUYESV0dXdFV1dnZXPrXjifytFIVl2VX26Hl1d35gsOyKifnBtsuxRhWKy7NqG6mTZQ+vrkmWv2bYjWfZrHemyCwnPvl9fnW5uRkQUqhrSZb+yIll2y6LWZNnHDBuULLtpxDHJskeMHJMs+4iEZ1VtHH5EsuyIiPr6wcmya+rSzZ9SXbrnnyike46o7k73gFiKjmTZkfB1UETCbRkRkfA5qFRK91qlUF/Z26WhtfKvvQ/bh6UCAAD0BUoQAACQFSUIAADIihIEAABkRQkCAACyogQBAABZUYIAAICsKEEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGRFCQIAALKiBAEAAFlRggAAgKwoQQAAQFaUIAAAICtKEAAAkBUlCAAAyIoSBAAAZEUJAgAAsqIEAQAAWanp7QH0R6UEmd0JMncqJqy6NQnvQkeOPiZZdkREx7aWZNmDtm1Jll2/fUey7IbadHeWIY0NybLbO7qSZXd3dybLbkx4e0dEDGkclCx73OgjkmWf0DwmWfbo5gnJsoeMGpcsu6GhMVl2dX1tsuzOKCTLjojoTvGEXJZu7NUpx50wu1hMd5sUqhK+JC2le44olpLeCaOYcoMmjC5U+MVnsbvy9z17ggAAgKwoQQAAQFaUIAAAICtKEAAAkBUlCAAAyIoSBAAAZEUJAgAAsqIEAQAAWVGCAACArChBAABAVpQgAAAgK0oQAACQFSUIAADIihIEAABkRQkCAACyogQBAABZUYIAAICsKEEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGSlprcH0N90RCE6olDx3O4EmTtVJey66UYd0TB0eML0iGFjjkmW3dW1Ill2qTtZdJTSRUdNXbqBl7rqk2UXS8Vk2cMH1ybLjog49ugjk2WPPX5isuwjj25Oll0/9Ihk2YXahnTZpXSzs7s73X28WEr5LBERhXTPb6WUj4idCR8PqxLeV5IlRyQcdlQlvB8Wkr4SiojulLd6OqUK31sqnRdhTxAAAJAZJQgAAMiKEgQAAGRFCQIAALKiBAEAAFlRggAAgKwoQQAAQFaUIAAAICtKEAAAkBUlCAAAyIoSBAAAZEUJAgAAsqIEAQAAWVGCAACArChBAABAVpQgAAAgK0oQAACQFSUIAADIihIEAABkRQkCAACyogQBAABZUYIAAICsKEEAAEBWanp7AP1NV7EYncViguR0fbSqUEqWXUw47mLiil4/fES67G2bkmWXqgrJsrsL6W70mprOZNlVhXS3yRGNTcmyxx07Lll2RMTo5gnJsgcPHZosu7oh3W0e1bXJoru70t3Hq6urk2VHwseUQintA3kp4WNWqZTuubO7mC47St3JohNOnygkfD1RSpmdcFNGRNLdFcUkr2f/S6GronHFQuXHak8QAACQFSUIAADIihIEAABkRQkCAACyogQBAABZUYIAAICsKEEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGRFCQIAALKiBAEAAFlRggAAgKwoQQAAQFaUIAAAICtKEAAAkBUlCAAAyIoSBAAAZEUJAgAAsqIEAQAAWVGCAACArNT09gD6m+7u7uju7q54bqFQqHjmTt1RTJedbthRqEoYHhF1jSOSZQ8ZdVyy7G2tLyTLjuodyaJH1NYmyz5yWGOy7KOOOSZZ9rCjm5NlR0TUDEp3u6T8G1pXKeHf5xI8fu+U8hGrVEoYHunCS2kHHlVV6e4rNdXpsotdHcmyO3e0Jsvesa0tWXZnR7rbpLujM1l2Z1dXsuyIiM5iyvx0j1r1NZWtGJtf3VzRvAh7ggAAgMwkK0FbtmyJb33rW/HJT34ypk+fHieccEIMGzYs6urqYvTo0XHuuefGbbfdFhs3bjygvEcffTRmz54d48ePj/r6+hg/fnzMnj07Hn300VS/AgAAMAAlOxxu0aJF8Sd/8id7vWzDhg2xYMGCWLBgQfzf//t/45vf/Ga8973v3eu6pVIpPvrRj8bcuXN3+/6aNWviwQcfjAcffDCuvfbauPvuu5MeUgYAAAwMSd8T1NzcHDNmzIjTTz89mpub4+ijj45isRirV6+O+fPnxwMPPBAtLS1xySWXxFNPPRWnnHLKHhmf/vSnywXotNNOi7/4i7+IiRMnxrJly+K2226LxYsXx9y5c2PUqFHx2c9+NuWvAwAADACFUqJ3LXZ3d0d1dXWP6zz00EMxa9asiIiYPXt2fPe7393t8qVLl8aUKVOiq6srzjjjjHj88cdj0KBB5ctbW1tj+vTp8fTTT0dNTU08//zzMXHixIr/LqtXr47m5tffwPzd//vpGH3E8Ir/jEIhXR+tTnmCgUK6t5UVCunewBwRUSimy9/26qvJste9lO7ECB3bNiXLbkz4BmYnRti7/npihFJNfbLspI9ZyZIjqqtT/s0y3ckLulOe5CIiqqrTnXClJuFjVn89MUJ3pxMjvJETI+xdpU+M8PKrm+PcG2+NiIhVq1bF+PHjDzkz2QzfXwGKiHj/+98fkydPjoiIxx9/fI/L77jjjuj6rzvXXXfdtVsBiogYPHhw3HXXXRER0dXVFXfeeechjhoAABjoev3scI2Nr/+lsr29fbfvl0ql+N73vhcREZMnT44zzzxzr9c/88wz46STToqI1/cspT4dJwAA0L/1aglasmRJ/Md//EdERHmP0E4rVqyINWvWRETE9OnTe8zZefnq1atj5cqVFR8nAAAwcBz2D0ttbW2NNWvWxA9+8IO47bbbyh88esMNN+y23pIlS8rLbyxIb7Tr5UuWLInjjju4D6pcvXp1j5evW7fuoPIAAIC+67CUoHnz5sXVV1+9z8tvuummuOKKK3b73qpVq8rL+3vz086TFrzxegdq1+sDAAAD22HfE7SrU089Ne6+++54+9vfvsdlW7duLS83NTX1mLPzfUUREdu2bavcAAEAgAHnsJSg97///XHGGWdERERbW1ssW7Ysvv3tb8eDDz4YV1xxRdx5551x8cUX73adXU+UUFdX12N+ff1/n061re3gT9+4v71H69ati2nTph10LgAA0PcclhI0fPjwGD58ePn/U6dOjQ9+8IPxjW98I6666qqYOXNm3HvvvTFnzpzyOg0NDeXljv2cO37Hjh3l5TeeRvtAVOJc4wAAQP/Qq2eHu/LKK+Oyyy6LYrEYn/jEJ2LTpv/+oMYhQ4aUl/d3iNv27dvLy/s7dA4AAMhbr39O0MyZMyPi9SLzr//6r+Xv77p3Zn9nb9v1cDYnOQAAAHrS6yVo1KhR5eWXXnqpvHzyySeXl59//vkeM3a9fMqUKRUcHQAAMND0egna+YGoEbsfynbcccfF2LFjIyJiwYIFPWY8/vjjERExbty4mDBhQuUHCQAADBi9XoK+853vlJff9ra3lZcLhUL5ULnnn38+nnzyyb1e/8knnyzvCZo5c2YUCoWEowUAAPq7ZCVo3rx5u53mem/uuOOO+OEPfxgRERMmTIh3vvOdu11+4403Rk3N6yewu/766/c4/XVbW1tcf/31ERFRU1MTN954Y4VGDwAADFTJTpH9f/7P/4lPfvKTcemll8Y73/nOmDhxYjQ1NcXWrVvj2WefjX/6p3+KJ554IiJe/xygr3zlK+XCs9OkSZPipptuiltvvTWefvrpOPvss+Pmm2+OiRMnxrJly+ILX/hCLF68OCIiPvWpT8WJJ56Y6tcBAAAGiKSfE/Tqq6/GV77ylfjKV76yz3XGjx8f9913X5x33nl7vfxzn/tcrF+/Pu67775YvHhxfPCDH9xjnWuuuSY++9nPVmzcAADAwJWsBP3bv/1b/OQnP4mf/vSnsWTJknjllVdi48aN0dDQEGPGjIlTTz01Lr744rj88stj8ODB+8ypqqqKe++9Ny699NKYO3duPPXUU9HS0hIjR46MqVOnxnXXXRcXXHBBql8DAAAYYJKVoIkTJ8bEiRPjuuuuq0jehRdeGBdeeGFFsg5J8b++Kh5bqnzozuyudNmF6mTRUahJGB4R0Z3uJBq1jSOSZY8Zle6zsNoaapNlD04XHcc0j0uW3XjUMcmyC9VJd8ZHVcITxZRSnoOmuyNddlW6O2KhKt32LBTSnceoqtidLDuKCbdlRHRs35ose/Pm15Jlt2/ftP+V3qTOtp4/YP5QdOzYkSy7vSNddsrXQaWEr98iIrq7O9OFlxI+HlZXNnvDlu0VzYvoA2eHAwAAOJyUIAAAICtKEAAAkBUlCAAAyIoSBAAAZEUJAgAAsqIEAQAAWVGCAACArChBAABAVpQgAAAgK0oQAACQFSUIAADIihIEAABkRQkCAACyogQBAABZUYIAAICsKEEAAEBWlCAAACArShAAAJAVJQgAAMhKTW8PoD/o6uoqL298bUuSn1GK6iS5qRWq0vXoQk0hWXZERHSVkkUXuzuTZXds3posu31ba7LsQbXJoqNuU7rbZFDNpmTZheq0D8HVhXRzqJQyu5Rw7hfS3eaFqnTZ1TUJx13sTpZdTJgdEdHR3pEsu33r5nTZrWleS0REdLWnexzv7NiRLHtHR7ptWexO93xfinTZERHd3V37X+nNKiV8XKmu7JP+xl1en+z6uvxQKEEHYMOGDeXla/+//79eHAkAAORrw4YNMWHChEPOcTgcAACQlUKpVEq7H28AaG9vj2effTYiIkaNGhU1PRyWsG7dupg2bVpERCxatCiOPvrowzJG0rA9Bw7bcmCxPQcO23JgsT0Hjr60Lbu6uspHZr3tbW+LhoaGQ850ONwBaGhoiKlTpx709Y4++ugYP358ghHRG2zPgcO2HFhsz4HDthxYbM+Boy9sy0ocArcrh8MBAABZUYIAAICsKEEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGTFh6UCAABZsScIAADIihIEAABkRQkCAACyogQBAABZUYIAAICsKEEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGRFCaqgP/zhD3HTTTfFlClTorGxMY444oiYNm1afPGLX4zW1tbeHh4HoFAoHNDXueee29tDzd769evj4YcfjltuuSUuuOCCGDlyZHn7zJkz56DzHn300Zg9e3aMHz8+6uvrY/z48TF79ux49NFHKz94dlOJbTlv3rwDnr/z5s1L+vvk7plnnonPf/7zccEFF0Rzc3PU19dHU1NTTJo0KebMmRM///nPDyrP3Ow9ldiW5mbfsGXLlvjWt74Vn/zkJ2P69OlxwgknxLBhw6Kuri5Gjx4d5557btx2222xcePGA8obEPOyREU8/PDDpWHDhpUiYq9fJ510UmnZsmW9PUz2Y1/b741f06dP7+2hZq+n7XPVVVcdcE6xWCxde+21PeZde+21pWKxmO6XyVwltuVXv/rVA56/X/3qV5P+Pjn74z/+4wPaBldeeWVpx44dPWaZm72rUtvS3OwbfvzjHx/QNhg5cmTp0Ucf3WfOQJqXNfsrSezff/7nf8bll18era2t0dTUFH/1V38VM2bMiLa2tvjWt74VX/nKV+L3v/99XHTRRfHUU09FU1NTbw+Z/fjYxz4Wf/qnf7rPyxsbGw/jaNif5ubmmDJlSjz22GMHfd1Pf/rTMXfu3IiIOO200+Iv/uIvYuLEibFs2bK47bbbYvHixTF37twYNWpUfPazn6300HmDQ9mWO/3oRz+KsWPH7vPy8ePHv+lserZmzZqIiBg7dmxcdtllcc4558QxxxwT3d3dsXDhwrj99ttjzZo18Y1vfCO6urrin//5n/eZZW72rkpuy53Mzd7V3NwcM2bMiNNPPz2am5vj6KOPjmKxGKtXr4758+fHAw88EC0tLXHJJZfEU089FaeccsoeGQNqXvZ2CxsIzj333FJElGpqakq//OUv97j8tttuK7fjv/u7v+uFEXKgdm6nz3zmM709FPbjlltuKf3gBz8ovfzyy6VSqVRasWLFQe89ePHFF0s1NTWliCidccYZpdbW1t0u3759e+mMM84oz++lS5dW+tegVJltuetfm1esWJFusPTooosuKv3Lv/xLqaura6+Xb9iwoTRp0qTytnr88cf3up652fsqtS3Nzb5hX9txVw8++GB5W82ePXuPywfavFSCDtGiRYvKd5jrrrtur+t0d3eXpkyZUoqI0ogRI0odHR2HeZQcKCWo/3ozL5z/9E//tHydhQsX7nWdhQsXltf5xCc+UcERsy9K0MD2gx/8oLyt/uzP/myv65ib/cOBbEtzs3+ZPHly+bC4Nxpo89KJEQ7RQw89VF6++uqr97pOVVVVfOhDH4qIiE2bNsXPfvazwzAyoCelUim+973vRUTE5MmT48wzz9zremeeeWacdNJJEfH6fC+VSodtjDAQ7XpimWXLlu1xubnZf+xvW9L/7Dzcv729fbfvD8R5qQQdop1nRmlsbIzTTz99n+tNnz69vPyLX/wi+biAnq1YsaJ8zPuu83Nvdl6+evXqWLlyZeqhwYDW0dFRXq6q2vNliLnZf+xvW9K/LFmyJP7jP/4jIl4vOrsaiPPSPfYQLVmyJCIiTjjhhKip2fd5Jna9M+28Dn3Xd77znTjppJNi0KBBMWTIkDjxxBPjqquuip/+9Ke9PTQqZNd5+MYH+zcyf/uXOXPmxJgxY6Kuri5GjhwZZ555Znz6058uP4HTuxYsWFBe3tvcMzf7j/1tyzcyN/ue1tbWePHFF+NLX/pSzJgxI7q7uyMi4oYbbthtvYE4L5WgQ9De3h4tLS0Rsf8zmowYMaK8i3HVqlXJx8ahee655+KFF16I9vb22LZtWyxdujS+/vWvx7ve9a6YNWtWbN68ubeHyCHadR7ub/42Nzfv9Xr0TQsWLIj169dHZ2dnbNy4MX71q1/F5z73uTjhhBPiH//xH3t7eFkrFotx6623lv9/+eWX77GOudk/HMi2fCNzs2/Y9bObGhsbY9KkSfHJT34yXnnllYiIuOmmm+KKK67Y7ToDcV46RfYh2Lp1a3n5QE573djYGNu3b49t27alHBaHYPDgwXHJJZfEu9/97pg8eXI0NTXFhg0bYsGCBXH33XfHxo0b46GHHoqZM2fGj3/846itre3tIfMmHcz83fWU6OZv33X88cfH7Nmz46yzzio/CS9fvjy++93vxvz586O9vT0++tGPRqFQiGuvvbaXR5unO+64IxYtWhQREbNmzYozzjhjj3XMzf7hQLblTuZm/3DqqafG3XffHW9/+9v3uGwgzksl6BDs+qaxurq6/a5fX18fERFtbW3JxsShWbNmTQwfPnyP759//vlx/fXXxwUXXBCLFy+OBQsWxJe//OX4sz/7s8M/SCriYObvzrkbYf72VbNmzYqrrroqCoXCbt+fOnVqfOADH4iHH344Zs+eHZ2dnfHnf/7ncckll8RRRx3VS6PN04IFC+Iv//IvIyJi9OjR8eUvf3mv65mbfd+BbssIc7Mvev/7318urW1tbbFs2bL49re/HQ8++GBcccUVceedd8bFF1+823UG4rx0ONwhaGhoKC/v+ubAfdmxY0dERAwaNCjZmDg0eytAO40ZMybmz59fnvx33XXXYRoVKRzM/N05dyPM375q2LBhe7zI2tXFF18cn/nMZyLi9WPg77333sM1NCLid7/7XcyaNSu6urqivr4+vv3tb8eYMWP2uq652bcdzLaMMDf7ouHDh8db3/rWeOtb3xpTp06ND37wg/HAAw/E17/+9Vi+fHnMnDkz5s2bt9t1BuK8VIIOwZAhQ8rLB7K7b/v27RFxYIfO0Tcdf/zxcf7550dExNKlS2Pt2rW9PCLerIOZvzvnboT525995CMfKb8Y2/UN3aS1YsWKeM973hObNm2K6urquP/++3s8u5S52Xcd7LY8UOZm33DllVfGZZddFsViMT7xiU/Epk2bypcNxHmpBB2ChoaGGDlyZES8fhrAnmzatKl8p9j1DWP0PyeffHJ52Rlt+q9d39i5v/m76xs7zd/+a/To0eXHbHP38Fi7dm2cd955sXbt2igUCnHffffFrFmzeryOudk3vZlteaDMzb5j5syZEfF6kfnXf/3X8vcH4rxUgg7RlClTIuL1vQJdXV37XO/555/f4zr0T335g784cLuW2V3n596YvwOH+Xv4tLS0xPnnnx/Lly+PiNcPId75weE9MTf7nje7LQ+Gudk3jBo1qrz80ksvlZcH4rxUgg7RO9/5zoh4vTH/+te/3ud6u+7ePfvss5OPi3See+658vLYsWN7cSQciuOOO668/fZ3+MXjjz8eERHjxo2LCRMmpB4aiaxfvz42btwYEeZuaps3b473vve95cfLW2+9NT7+8Y8f0HXNzb7lULblgTI3+45d98TteijbQJyXStAhev/7319e/upXv7rXdYrFYnz961+PiNffjDZjxozDMTQSWL58efz4xz+OiNffHzRu3LheHhFvVqFQKO/2f/755+PJJ5/c63pPPvlk+a9aM2fO7PENvvRtc+fOLf+1uRLvY2DvWltb46KLLopnnnkmIiL+5m/+Jm6++eYDvr652Xcc6rY8UOZm3/Gd73ynvPy2t72tvDwg52WJQ3bOOeeUIqJUU1NT+uUvf7nH5bfddlspIkoRUfrMZz5z+AfIAfn+979f6uzs3OflL7/8cum0004rb8vbb7/9MI6O/VmxYkV521x11VUHdJ3f//73pZqamlJElM4444xSa2vrbpe3traWzjjjjPL8fuGFFxKMnDc62G25YsWK0jPPPNPjOj/4wQ9KdXV1pYgoNTQ0lFavXl2h0bKrHTt2lN7znveUt98NN9zwpnLMzd5XiW1pbvYdX/3qV0ttbW09rvOlL32pvL0nTJiwx2uigTYvC6WSgzAP1eLFi+Pss8+Otra2aGpqir/+67+OGTNmRFtbW3zrW9+KuXPnRkTEpEmT4umnn97tDBv0HRMmTIjOzs649NJL46yzzooJEybEoEGDoqWlJX72s5+VPyw14vXDIH/yk5/sdi58Dq9f/OIXsXTp0vL/W1pa4lOf+lREvH7I6Yc//OHd1p8zZ85ec/7qr/6q/Knnp512Wtx8880xceLEWLZsWXzhC1+IxYsXl9f7/Oc/n+A34VC35c9+9rOYMWNGnHXWWfG+970vTj311Bg9enSUSqVYvnx5zJ8/P+bPn1/+S/P/+3//r+KH8/C6Sy+9NB544IGIiHjXu94Vd955Z49/Ca6rq4tJkybt9TJzs3dVYluam33HhAkTYuvWrXHppZfGO9/5zpg4cWI0NTXF1q1b49lnn41/+qd/iieeeCIiXt+WjzzySJx33nl75AyoedmbDWwg+f73v18aOnRouUG/8WvSpEmlF198sbeHSQ+OPfbYfW6/Xb8uvfTS0qZNm3p7uNm76qqrDmh77fzal+7u7tL//t//u8frXnPNNaXu7u7D+Nvl5VC35U9/+tMDut7gwYNL//iP/9gLv2E+DmY7RkTp2GOP3WeWudm7KrEtzc2+40Bf44wfP7702GOP7TNnIM1Le4Iq6KWXXop/+Id/iEceeSRWr14ddXV1ccIJJ8Rll10Wn/jEJ2Lw4MG9PUR6sGDBgliwYEEsXLgwli9fHi0tLbFly5ZoamqK5ubmeMc73hFXXXVVnHXWWb09VOL1vQFf+9rXDnj9/T3U/fCHP4y5c+fGU089FS0tLTFy5MiYOnVqXHfddXHBBRcc6nDpwaFuy61bt8b3v//9WLhwYTz99NOxbt26aGlpia6urhgxYkS85S1viXe/+93x4Q9/OEaPHl3p4bOLgz3+/9hjj42VK1f2uI652TsqsS3Nzb5j2bJl8ZOf/CR++tOfxpIlS+KVV16JjRs3RkNDQ4wZMyZOPfXUuPjii+Pyyy8/oNerA2FeKkEAAEBWnB0OAADIihIEAABkRQkCAACyogQBAABZUYIAAICsKEEAAEBWlCAAACArShAAAJAVJQgAAMiKEgQAAGRFCQIAALKiBAEAAFlRggAAgKwoQQAAQFaUIAAAICtKEAAAkBUlCAAAyIoSBAAAZEUJAgAAsqIEAQAAWVGCAACArChBAABAVpQgAAAgK0oQAACQlf8/BYpf7ZANwScAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 413,
       "width": 416
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_example(*dataset[5499])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Training and Validation Sets\n",
    "\n",
    "We need make sure we have sets for training, validation and testing. The test set was already done as part of the download. But we can pull out 5000 images from the training set randomly and use that for the validation during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 5000)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_size = 5000\n",
    "train_size = len(dataset) - validation_size\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, validation_size])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the data loaders for each of the sets (train_ds and val_ds), so that we can load data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "trainloader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "validloader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the CNN Model\n",
    "\n",
    "Now that data is ready, we can define the model.\n",
    "\n",
    "Some notes about planning this out:\n",
    "* Since our images have 3 channels, we need a kernel for each channel and keeping in mind that the outputs are added together pixel-wise. We'll use Conv2d to transform our 3-channel images to a 16-channel feature map.\n",
    "* The model needs to have max-pooling layers to reduce the sizes of the output from each convolutional layer. We'll use MaxPool2d which will cut the feature map in half each time.\n",
    "* Finally, we'll add fully connected layers at the end to serve as the classifier part of the model which will take the output of the convolutional part of the model and give us the vector of size 10 (1 per class).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify our loss function and optimizer\n",
    "\n",
    "learn_rate = 0.0001\n",
    "\n",
    "# Loss function:\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer:\n",
    "optimizer = optim.SGD(model.parameters(), lr=learn_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your Neural Network\n",
    "Using the layers in `torch.nn` (which has been imported as `nn`) and the `torch.nn.functional` module (imported as `F`), construct a neural network based on the parameters of the dataset. \n",
    "Feel free to construct a model of any architecture – feedforward, convolutional, or even something more advanced!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model & Helper Functions\n",
    "We'll define the model by extending an `ImageClassificationBase`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationBase(nn.Module):\n",
    "    # This function executes a training step by generating predictions and calculating loss\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        return loss\n",
    "    \n",
    "    # This function executes the validation step by generating predictions and calculating loss\n",
    "    # on the validation data. It also calculates the accuracy.\n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)                    \n",
    "        loss = F.cross_entropy(out, labels)   \n",
    "        acc = accuracy(out, labels)           \n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc} #NOTE: .detach() just keeps the tensor, no gradient info\n",
    "\n",
    "    # This function puts together all the values from each batch: the losses and accuracy, so we have\n",
    "    # just one combined value per epoch.\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    # This function prints out the model performance for each epoch so we can track it:\n",
    "    def epoch_end(self, epochs, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epochs, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "\n",
    "        # This function gives an overall percentage accuracy for the model (percent of predictions that were right)\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put together all the layers into one Network\n",
    "\n",
    "this network architecture is attributed to excellent tutorial by Shadab Hussain on Kaggle [here](https://www.kaggle.com/code/shadabhussain/cifar-10-cnn-using-pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TutorialModel(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU()\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Flatten(start_dim=1, end_dim=-1)\n",
       "    (16): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (17): ReLU()\n",
       "    (18): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (19): ReLU()\n",
       "    (20): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TutorialModel(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)\n",
    "    \n",
    "#''''''Look at the model:''''''\n",
    "model = TutorialModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: torch.Size([128, 3, 32, 32])\n",
      "out.shape: torch.Size([128, 10])\n",
      "out[0]: tensor([ 0.0260, -0.0270, -0.0406, -0.0263, -0.0117,  0.0291, -0.0372, -0.0201,\n",
      "         0.0160, -0.0046], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Checking that the model output is in the shape we expect.\n",
    "# each batch from the trainloader has 128 images and each is a 3, 32, 32 tensor.\n",
    "# the model should output a vector with 10 elements\n",
    "for images, labels in trainloader:\n",
    "    print('images.shape:', images.shape)\n",
    "    out = model(images)\n",
    "    print('out.shape:', out.shape)\n",
    "    print('out[0]:', out[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify a loss function and an optimizer, and instantiate the model.\n",
    "\n",
    "If you use a less common loss function, please note why you chose that loss function in a comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running your Neural Network\n",
    "Use whatever method you like to train your neural network, and ensure you record the average loss at each epoch. \n",
    "Don't forget to use `torch.device()` and the `.to()` method for both your model and your data if you are using GPU!\n",
    "\n",
    "If you want to print your loss during each epoch, you can use the `enumerate` function and print the loss after a set number of batches. 250 batches works well for most people!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapping the data and the model reliably based on where the notebook will run:\n",
    "Handy borrowed helpers to make sure the data and model get moved to GPU (or not) as needed.\n",
    "This code came from excellent tutorial by Shadab Husain on Kaggle [here](https://www.kaggle.com/code/shadabhussain/cifar-10-cnn-using-pytorch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# set device based on where the notebook is running:\n",
    "device = get_default_device()\n",
    "print(device)\n",
    "\n",
    "\n",
    "# Wrap the data loaders so that the data and models get moved to the appropriate place:\n",
    "trainloader = DeviceDataLoader(trainloader, device)\n",
    "validloader = DeviceDataLoader(validloader, device)\n",
    "to_device(model, device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "Recall the helper functions we created when defining the model:\n",
    "* training_step(self, batch) - takes the model and batch of images and returns loss\n",
    "* validation_step(self, batch) - takes the model and batch of images and returns loss and accuracy for the step\n",
    "* validation_epoch_end(self, outputs) - combines the losses and accuracy scores for the whole epoch\n",
    "* epoch_end(self, epoch, result) - prints out the epoch's training loss, validation loss and accuracy score\n",
    "* accuracy(outputs, labels) - compares the prediction to the actual label and computes the average number that were correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function called evaluate that will be used to take a batch from validation data,\n",
    "# apply the model and return the loss and accuracy for the step and for the epoch overall\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "# Define a function called fit that will be used for training/validation/seeing training progress:\n",
    "def fit(epochs, learn_rate, model, trainloader, validloader, optimizer):\n",
    "    progress = []\n",
    "    optimizer = optimizer\n",
    "    for e in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in trainloader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation Phase\n",
    "        result = evaluate(model, validloader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epochs, result)\n",
    "        progress.append(result)\n",
    "    return progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Function:  CrossEntropyLoss()\n",
      "Optimization Function:  SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Learning Rate:  0.0001\n"
     ]
    }
   ],
   "source": [
    "model = to_device(TutorialModel(), device)\n",
    "\n",
    "#What are our hp's (just to minimize scrolling)\n",
    "print('Loss Function: ', criterion)\n",
    "print('Optimization Function: ', optimizer)\n",
    "print('Learning Rate: ', learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial model performance\n",
    "Before the training step, see how model performs using the validation set with initial parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "model = to_device(TutorialModel(), device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.3029215335845947, 'val_acc': 0.10629595816135406}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, validloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the untrained model's accuracy is about 10% - same as random guessing.\n",
    "\n",
    "Now we can train the model for real. For now, we'll keep the same hyperparameters as are already defined. If the results aren't good, we can change them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "learn_rate = 0.001\n",
    "\n",
    "# Loss function:\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer:\n",
    "optimizer = optim.Adam(model.parameters(), lr=learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10], train_loss: 1.7048, val_loss: 1.3128, val_acc: 0.5207\n",
      "Epoch [10], train_loss: 1.1613, val_loss: 1.0470, val_acc: 0.6294\n",
      "Epoch [10], train_loss: 0.8853, val_loss: 0.8618, val_acc: 0.6997\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pj/w4qst4gx3n71p8wgs8krz2480000gn/T/ipykernel_18828/2380525417.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprogress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/pj/w4qst4gx3n71p8wgs8krz2480000gn/T/ipykernel_18828/958814549.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn_rate, model, trainloader, validloader, optimizer)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "progress = fit(epochs, learn_rate, model, trainloader, validloader, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training loss (and validation loss/accuracy, if recorded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracies = [x['val_acc'] for x in progress]\n",
    "plt.plot(accuracies, '-x')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy vs. No. of epochs');\n",
    "\n",
    "\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your model\n",
    "Using the previously created `DataLoader` for the test set, compute the percentage of correct predictions using the highest probability prediction. \n",
    "\n",
    "If your accuracy is over 70%, great work! \n",
    "This is a hard task to exceed 70% on.\n",
    "\n",
    "If your accuracy is under 45%, you'll need to make improvements.\n",
    "Go back and check your model architecture, loss function, and optimizer to make sure they're appropriate for an image classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving your model\n",
    "Using `torch.save`, save your model for future loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Recommendation\n",
    "\n",
    "Based on your evaluation, what is your recommendation on whether to build or buy? Explain your reasoning below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Double click this cell to modify it**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
