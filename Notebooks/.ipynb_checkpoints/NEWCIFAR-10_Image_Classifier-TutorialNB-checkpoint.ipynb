{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this project, you will build a neural network of your own design to evaluate the CIFAR-10 dataset.\n",
    "Our target accuracy is 70%, but any accuracy over 50% is a great start.\n",
    "Some of the benchmark results on CIFAR-10 include:\n",
    "\n",
    "78.9% Accuracy | [Deep Belief Networks; Krizhevsky, 2010](https://www.cs.toronto.edu/~kriz/conv-cifar10-aug2010.pdf)\n",
    "\n",
    "90.6% Accuracy | [Maxout Networks; Goodfellow et al., 2013](https://arxiv.org/pdf/1302.4389.pdf)\n",
    "\n",
    "96.0% Accuracy | [Wide Residual Networks; Zagoruyko et al., 2016](https://arxiv.org/pdf/1605.07146.pdf)\n",
    "\n",
    "99.0% Accuracy | [GPipe; Huang et al., 2018](https://arxiv.org/pdf/1811.06965.pdf)\n",
    "\n",
    "98.5% Accuracy | [Rethinking Recurrent Neural Networks and other Improvements for ImageClassification; Nguyen et al., 2020](https://arxiv.org/pdf/2007.15161.pdf)\n",
    "\n",
    "Research with this dataset is ongoing. Notably, many of these networks are quite large and quite expensive to train. \n",
    "\n",
    "## Citations\n",
    "\n",
    "Dataset: This tech report (Chapter 3) describes the dataset and the methodology followed when collecting it in much greater detail. Please cite it if you intend to use this dataset.\n",
    "[Learning Multiple Layers of Features from Tiny Images](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf), Alex Krizhevsky, 2009.\n",
    "\n",
    "[Normalization values for transforms:](https://stackoverflow.com/questions/66678052/how-to-calculate-the-mean-and-the-std-of-cifar10-data)\n",
    "\n",
    "[Transfer Learning for Computer Vision Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) - was referred by a very nice technical mentor I reached out to after going crazy all weekend with a feed-forward model that would not learn! :)\n",
    "\n",
    "\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell contains the essential imports you will need â€“ DO NOT CHANGE THE CONTENTS! ##\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available.  Training on CPU ...\n"
     ]
    }
   ],
   "source": [
    "# My additional imports and magics & stuff\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "import tarfile\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Specify your transforms as a list first.\n",
    "The transforms module is already loaded as `transforms`.\n",
    "\n",
    "CIFAR-10 is fortunately included in the torchvision module.\n",
    "Then, you can create your dataset using the `CIFAR10` object from `torchvision.datasets` ([the documentation is available here](https://pytorch.org/docs/stable/torchvision/datasets.html#cifar)).\n",
    "Make sure to specify `download=True`! \n",
    "\n",
    "Once your dataset is created, you'll also need to define a `DataLoader` from the `torch.utils.data` module for both the train and the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I'm struggling to put this all together, let's start with images themselves and remove the complexity of using these batches and pytorch objects, as they are just one more thing that can go wrong and aren't necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz to ./cifar10.tgz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd33d88f784c422aa3787d2ea86e3820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135107811 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dowload the dataset\n",
    "dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\n",
    "download_url(dataset_url, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data from tar file:\n",
    "with tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n",
    "    tar.extractall(path='./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'test', 'train']\n",
      "['cat', '.DS_Store', 'dog', 'truck', 'bird', 'airplane', 'ship', 'frog', 'horse', 'deer', 'automobile']\n"
     ]
    }
   ],
   "source": [
    "# verifying the data is there:\n",
    "\n",
    "data_dir = './data/cifar10'\n",
    "\n",
    "print(os.listdir(data_dir))\n",
    "classes_list = os.listdir(data_dir + \"/train\")\n",
    "print(classes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of test examples for ship: 1000\n",
      "['0298.png', '0267.png', '0501.png', '0515.png', '0273.png']\n"
     ]
    }
   ],
   "source": [
    "ship_test_files = os.listdir(data_dir + \"/test/ship\")\n",
    "print(\"No. of test examples for ship:\", len(ship_test_files))\n",
    "print(ship_test_files[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(data_dir+'/train', transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element of this dataset contains an image tensor and a label. The images are 3-channels with dimensions of 32x32, so the expected tensor shape for each image will 3, 32, 32. The labels should just be a 1-dimensional tensor, but let's pick out an element to check that this is what we really have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32]) 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4431, 0.4314, 0.4353,  ..., 0.6039, 0.6431, 0.6745],\n",
       "         [0.4471, 0.4314, 0.4353,  ..., 0.6667, 0.6235, 0.6314],\n",
       "         [0.4588, 0.4549, 0.4471,  ..., 0.4902, 0.4471, 0.5294],\n",
       "         ...,\n",
       "         [0.2039, 0.2039, 0.2000,  ..., 0.6902, 0.6745, 0.6784],\n",
       "         [0.1765, 0.1804, 0.2000,  ..., 0.6745, 0.6784, 0.6863],\n",
       "         [0.1882, 0.2000, 0.2235,  ..., 0.6784, 0.6745, 0.6863]],\n",
       "\n",
       "        [[0.3843, 0.3725, 0.3725,  ..., 0.5961, 0.6431, 0.6745],\n",
       "         [0.3804, 0.3686, 0.3725,  ..., 0.6549, 0.6196, 0.6275],\n",
       "         [0.3765, 0.3804, 0.3765,  ..., 0.4784, 0.4392, 0.5216],\n",
       "         ...,\n",
       "         [0.2039, 0.1961, 0.1843,  ..., 0.6627, 0.6549, 0.6588],\n",
       "         [0.1882, 0.1804, 0.1961,  ..., 0.6510, 0.6549, 0.6667],\n",
       "         [0.2039, 0.2039, 0.2235,  ..., 0.6588, 0.6510, 0.6667]],\n",
       "\n",
       "        [[0.2706, 0.2588, 0.2627,  ..., 0.6078, 0.6510, 0.6824],\n",
       "         [0.2706, 0.2627, 0.2706,  ..., 0.6667, 0.6314, 0.6392],\n",
       "         [0.2588, 0.2745, 0.2706,  ..., 0.4902, 0.4510, 0.5333],\n",
       "         ...,\n",
       "         [0.1922, 0.2039, 0.2078,  ..., 0.6941, 0.6863, 0.6902],\n",
       "         [0.1843, 0.1843, 0.2078,  ..., 0.6784, 0.6863, 0.6941],\n",
       "         [0.2000, 0.2039, 0.2235,  ..., 0.6863, 0.6784, 0.6902]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = dataset[7]\n",
    "print(img.shape, label)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.classes) #this is a property of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Dataset\n",
    "Using matplotlib, numpy, and torch, explore the dimensions of your data.\n",
    "\n",
    "You can view images using the `show5` function defined below â€“ it takes a data loader as an argument.\n",
    "Remember that normalized images will look really weird to you! You may want to try changing your transforms to view images.\n",
    "Typically using no transforms other than `toTensor()` works well for viewing â€“ but not as well for training your network.\n",
    "If `show5` doesn't work, go back and check your code for creating your data loaders and your training/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to change the image tensor to (32, 32, 3) for viewing and also adds the label and displayes the image.\n",
    "def show_example(img, label):\n",
    "    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n",
    "    plt.imshow(img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  automobile (1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 413,
       "width": 416
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_example(*dataset[5499])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Training and Validation Sets\n",
    "\n",
    "We need make sure we have sets for training, validation and testing. The test set was already done as part of the download. But we can pull out 5000 images from the training set randomly and use that for the validation during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 5000)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_size = 5000\n",
    "train_size = len(dataset) - validation_size\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, validation_size])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the data loaders for each of the sets (train_ds and val_ds), so that we can load data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "trainloader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "validloader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the CNN Model\n",
    "\n",
    "Now that data is ready, we can define the model.\n",
    "\n",
    "Some notes about planning this out:\n",
    "* Since our images have 3 channels, we need a kernel for each channel and keeping in mind that the outputs are added together pixel-wise. We'll use Conv2d to transform our 3-channel images to a 16-channel feature map.\n",
    "* The model needs to have max-pooling layers to reduce the sizes of the output from each convolutional layer. We'll use MaxPool2d which will cut the feature map in half each time.\n",
    "* Finally, we'll add fully connected layers at the end to serve as the classifier part of the model which will take the output of the convolutional part of the model and give us the vector of size 10 (1 per class).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify our loss function and optimizer\n",
    "\n",
    "learn_rate = 0.0001\n",
    "\n",
    "# Loss function:\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer:\n",
    "optimizer = optim.SGD(model.parameters(), lr=learn_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your Neural Network\n",
    "Using the layers in `torch.nn` (which has been imported as `nn`) and the `torch.nn.functional` module (imported as `F`), construct a neural network based on the parameters of the dataset. \n",
    "Feel free to construct a model of any architecture â€“ feedforward, convolutional, or even something more advanced!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model & Helper Functions\n",
    "We'll define the model by extending an `ImageClassificationBase`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationBase(nn.Module):\n",
    "    # This function executes a training step by generating predictions and calculating loss\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        return loss\n",
    "    \n",
    "    # This function executes the validation step by generating predictions and calculating loss\n",
    "    # on the validation data. It also calculates the accuracy.\n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)                    \n",
    "        loss = F.cross_entropy(out, labels)   \n",
    "        acc = accuracy(out, labels)           \n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "\n",
    "    # This function puts together all the values from each batch: the losses and accuracy, so we have\n",
    "    # just one combined value per epoch.\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    # This function prints out the model performance for each epoch so we can track it:\n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "\n",
    "        # This function gives an overall percentage accuracy for the model (percent of predictions that were right)\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put together all the layers into one Network\n",
    "\n",
    "this network architecture is attributed to excellent tutorial by Shadab Hussain on Kaggle [here](https://www.kaggle.com/code/shadabhussain/cifar-10-cnn-using-pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TutorialModel(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU()\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Flatten(start_dim=1, end_dim=-1)\n",
       "    (16): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (17): ReLU()\n",
       "    (18): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (19): ReLU()\n",
       "    (20): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TutorialModel(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)\n",
    "    \n",
    "#''''''Look at the model:''''''\n",
    "model = TutorialModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify a loss function and an optimizer, and instantiate the model.\n",
    "\n",
    "If you use a less common loss function, please note why you chose that loss function in a comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running your Neural Network\n",
    "Use whatever method you like to train your neural network, and ensure you record the average loss at each epoch. \n",
    "Don't forget to use `torch.device()` and the `.to()` method for both your model and your data if you are using GPU!\n",
    "\n",
    "If you want to print your loss during each epoch, you can use the `enumerate` function and print the loss after a set number of batches. 250 batches works well for most people!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handy borrowed helpers to make sure the data and model get moved to GPU (or not) as needed.\n",
    "This code came from excellent tutorial by Shadab Husain on Kaggle [here](https://www.kaggle.com/code/shadabhussain/cifar-10-cnn-using-pytorch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device based on where the notebook is running:\n",
    "device = get_default_device()\n",
    "device\n",
    "\n",
    "# Wrap the data loaders so that the data and models get moved to the appropriate place:\n",
    "trainloader = DeviceDataLoader(trainloader, device)\n",
    "validloader = DeviceDataLoader(validloader, device)\n",
    "to_device(model, device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: torch.Size([128, 3, 32, 32])\n",
      "out.shape: torch.Size([128, 10])\n",
      "out[0]: tensor([ 0.0438, -0.0268, -0.0092,  0.0336,  0.0270,  0.0127, -0.0411,  0.0360,\n",
      "         0.0166, -0.0212], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in trainloader:\n",
    "    print('images.shape:', images.shape)\n",
    "    out = model(images)\n",
    "    print('out.shape:', out.shape)\n",
    "    print('out[0]:', out[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15..  Training Loss: 2.306..  Test Loss: 0.471..  Test Accuracy:  0.021.. \n",
      "Epoch: 2/15..  Training Loss: 2.306..  Test Loss: 0.471..  Test Accuracy:  0.021.. \n",
      "Epoch: 3/15..  Training Loss: 2.306..  Test Loss: 0.471..  Test Accuracy:  0.021.. \n",
      "Epoch: 4/15..  Training Loss: 2.306..  Test Loss: 0.471..  Test Accuracy:  0.021.. \n",
      "Epoch: 5/15..  Training Loss: 2.306..  Test Loss: 0.471..  Test Accuracy:  0.021.. \n",
      "Epoch: 6/15..  Training Loss: 2.306..  Test Loss: 0.471..  Test Accuracy:  0.021.. \n",
      "Epoch: 7/15..  Training Loss: 2.306..  Test Loss: 0.471..  Test Accuracy:  0.021.. \n",
      "Epoch: 8/15..  Training Loss: 2.306..  Test Loss: 0.471..  Test Accuracy:  0.021.. \n",
      "Epoch: 9/15..  Training Loss: 2.306..  Test Loss: 0.471..  Test Accuracy:  0.021.. \n",
      "Epoch: 10/15..  Training Loss: 2.306..  Test Loss: 0.471..  Test Accuracy:  0.021.. \n",
      "Epoch: 11/15..  Training Loss: 2.306..  Test Loss: 0.471..  Test Accuracy:  0.021.. \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pj/w4qst4gx3n71p8wgs8krz2480000gn/T/ipykernel_50864/1929378165.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# Feed test batch into model:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mlog_ps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_ps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \"\"\"\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;31m# put it from HWC to CHW format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ##\n",
    "model = Network()\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        images, labels = data\n",
    "    #for images, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        ## TODO: Implement the validation pass and print out the validation accuracy\n",
    "        ## For each training pass, you do a validation pass!!\n",
    "        \n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        ## Turn off gradients - only needed for training\n",
    "        with torch.no_grad():\n",
    "            # set model to evaluation mode\n",
    "            model.eval()\n",
    "            # Feed test batch into model:\n",
    "            for images, labels in testloader:\n",
    "                log_ps = model(images)\n",
    "                test_loss += criterion(log_ps, labels)\n",
    "            \n",
    "                # Get the class proabilities:\n",
    "                ps = torch.exp(log_ps)\n",
    "                # Get the class with the highest probability for each sample:\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "\n",
    "                # Get a tensor with the correct predictions by comparing top_class to the labels:\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "        \n",
    "                # Calculate the accuracy by taking the mean of our new equals tensor - after floatifying it\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                \n",
    "        # set model back to train mode\n",
    "        model.train()\n",
    "        \n",
    "        # Update the train_losses and test_losses lists:\n",
    "        train_losses.append(running_loss/len(trainloader))\n",
    "        test_losses.append(test_loss/len(testloader))\n",
    "        \n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "             \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader)),\n",
    "             \"Test Loss: {:.3f}.. \".format(test_loss/len(trainloader)),\n",
    "             \"Test Accuracy:  {:.3f}.. \".format(accuracy/len(trainloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training loss (and validation loss/accuracy, if recorded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your model\n",
    "Using the previously created `DataLoader` for the test set, compute the percentage of correct predictions using the highest probability prediction. \n",
    "\n",
    "If your accuracy is over 70%, great work! \n",
    "This is a hard task to exceed 70% on.\n",
    "\n",
    "If your accuracy is under 45%, you'll need to make improvements.\n",
    "Go back and check your model architecture, loss function, and optimizer to make sure they're appropriate for an image classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving your model\n",
    "Using `torch.save`, save your model for future loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Recommendation\n",
    "\n",
    "Based on your evaluation, what is your recommendation on whether to build or buy? Explain your reasoning below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Double click this cell to modify it**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
